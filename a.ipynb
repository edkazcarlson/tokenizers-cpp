{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "756930ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"TF_ENABLE_ONEDNN_OPTS\"] = \"0\"\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "import open_clip\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import random\n",
    "import pytorch_lightning as pl\n",
    "import argparse\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3ea9685e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = open_clip.get_tokenizer('ViT-B-16-SigLIP-512')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89b753e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  347,   269,   260,  1914,   267, 12706,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tensor([[ 262,  302,  491,  320, 2041,  262,  331, 7962,  432,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
      "            1,    1,    1,    1]])\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer(\"What is the capital of Canada?\"))\n",
    "print(tokenizer(\"I love my wife Ritsu\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2ea4703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('myExportTest\\\\tokenizer_config.json',\n",
       " 'myExportTest\\\\special_tokens_map.json',\n",
       " 'myExportTest\\\\tokenizer.json')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.tokenizer.save_pretrained('myExportTest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "304507dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.models.t5.tokenization_t5_fast.T5TokenizerFast"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(tokenizer.tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "adfdee67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With special tokens: [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "Length: 8\n",
      "No padding: [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "Length: 8\n",
      "add_special_tokens = False: [496, 269, 260, 1914, 267, 1717, 308]\n",
      "Length: 7\n",
      "tokenizer() call: tensor([[  347,   269,   260,  1914,   267, 12706,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "tokenizer() ids: [[347, 269, 260, 1914, 267, 12706, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Test encoding without padding/truncation\n",
    "result = tokenizer.tokenizer.encode(\"What is the capital of Canada?\", add_special_tokens=True)\n",
    "print(\"With special tokens:\", result)\n",
    "print(\"Length:\", len(result))\n",
    "\n",
    "# Test encoding with padding disabled\n",
    "result_no_pad = tokenizer.tokenizer.encode(\"What is the capital of Canada?\", add_special_tokens=True, padding=False)\n",
    "print(\"No padding:\", result_no_pad)\n",
    "print(\"Length:\", len(result_no_pad))\n",
    "\n",
    "\n",
    "result_no_pad = tokenizer.tokenizer.encode(\"What is the capital of Canada?\", add_special_tokens=False)\n",
    "print(f\"add_special_tokens = False: {result_no_pad}\")\n",
    "print(\"Length:\", len(result_no_pad))\n",
    "\n",
    "# Test what tokenizer() does (this is what you used)\n",
    "result_call = tokenizer(\"What is the capital of Canada?\")\n",
    "print(\"tokenizer() call:\", result_call)\n",
    "print(\"tokenizer() ids:\", result_call.tolist() if hasattr(result_call, 'tolist') else result_call)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "efb8dfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct encode: [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "First 10 tokens: [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "\n",
      "OpenCLIP tokenizer result: tensor([[  347,   269,   260,  1914,   267, 12706,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "             1,     1,     1,     1]])\n",
      "OpenCLIP ids (first 10): [[347, 269, 260, 1914, 267, 12706, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n"
     ]
    }
   ],
   "source": [
    "# Check what the actual tokenizer.encode() returns without any wrapper\n",
    "from transformers import AutoTokenizer\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('myExportTest')\n",
    "result_direct = loaded_tokenizer.encode(\"What is the capital of Canada?\", add_special_tokens=True)\n",
    "print(\"Direct encode:\", result_direct)\n",
    "print(\"First 10 tokens:\", result_direct[:10])\n",
    "\n",
    "# Compare with open_clip's tokenizer\n",
    "result_clip = tokenizer(\"What is the capital of Canada?\")\n",
    "print(\"\\nOpenCLIP tokenizer result:\", result_clip)\n",
    "if hasattr(result_clip, 'tolist'):\n",
    "    print(\"OpenCLIP ids (first 10):\", result_clip.tolist()[:10])\n",
    "elif isinstance(result_clip, dict):\n",
    "    print(\"OpenCLIP ids (first 10):\", result_clip['input_ids'][:10] if 'input_ids' in result_clip else result_clip)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0d338a7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token 347: ['▁what']\n",
      "Token 496: ['▁What']\n",
      "Token 12706: ['▁canada']\n",
      "Token 1717: ['▁Canada']\n",
      "Token 308: ['?']\n",
      "\n",
      "Full encoding of 'What is the capital of Canada?':\n",
      "Token IDs: [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "Tokens: ['▁What', '▁is', '▁the', '▁capital', '▁of', '▁Canada', '?', '</s>']\n"
     ]
    }
   ],
   "source": [
    "# Check what tokens 347, 496, 12706, 1717, and 308 represent\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained('myExportTest')\n",
    "print(\"Token 347:\", loaded_tokenizer.convert_ids_to_tokens([347]))\n",
    "print(\"Token 496:\", loaded_tokenizer.convert_ids_to_tokens([496]))\n",
    "print(\"Token 12706:\", loaded_tokenizer.convert_ids_to_tokens([12706]))\n",
    "print(\"Token 1717:\", loaded_tokenizer.convert_ids_to_tokens([1717]))\n",
    "print(\"Token 308:\", loaded_tokenizer.convert_ids_to_tokens([308]))\n",
    "\n",
    "# Also check the full encoding to see the difference\n",
    "text = \"What is the capital of Canada?\"\n",
    "result = loaded_tokenizer.encode(text, add_special_tokens=True)\n",
    "tokens = loaded_tokenizer.convert_ids_to_tokens(result)\n",
    "print(f\"\\nFull encoding of '{text}':\")\n",
    "print(\"Token IDs:\", result[:10])\n",
    "print(\"Tokens:\", tokens[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3916c5ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct encode('What...'): [496, 269, 260, 1914, 267, 1717, 308, 1]\n",
      "Direct encode('what...'): [347, 269, 260, 1914, 267, 12706, 308, 1]\n",
      "Are they different? True\n",
      "\n",
      "Wrapper('What...'): [347, 269, 260, 1914, 267, 12706, 1, 1]\n",
      "Wrapper('what...'): [347, 269, 260, 1914, 267, 12706, 1, 1]\n",
      "Are they the same? True\n"
     ]
    }
   ],
   "source": [
    "# Test to confirm: open_clip tokenizer lowercases the input\n",
    "text1 = \"What is the capital of Canada?\"\n",
    "text2 = \"what is the capital of canada?\"\n",
    "\n",
    "# Direct tokenizer encode (no preprocessing)\n",
    "result1_direct = tokenizer.tokenizer.encode(text1, add_special_tokens=True)\n",
    "result2_direct = tokenizer.tokenizer.encode(text2, add_special_tokens=True)\n",
    "print(\"Direct encode('What...'):\", result1_direct)\n",
    "print(\"Direct encode('what...'):\", result2_direct)\n",
    "print(\"Are they different?\", result1_direct != result2_direct)\n",
    "\n",
    "# open_clip wrapper (with preprocessing)\n",
    "result1_wrapper = tokenizer(text1)\n",
    "result2_wrapper = tokenizer(text2)\n",
    "print(\"\\nWrapper('What...'):\", result1_wrapper.tolist()[0][:8] if hasattr(result1_wrapper, 'tolist') else result1_wrapper)\n",
    "print(\"Wrapper('what...'):\", result2_wrapper.tolist()[0][:8] if hasattr(result2_wrapper, 'tolist') else result2_wrapper)\n",
    "print(\"Are they the same?\", (result1_wrapper.tolist()[0][:8] == result2_wrapper.tolist()[0][:8] if hasattr(result1_wrapper, 'tolist') else result1_wrapper == result2_wrapper))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc15117",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
